# -*- coding: utf-8 -*--
"""DataScienceFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IxWeotPUMf4MmikJH_6t3wXDlFEdKBzs
"""

import pandas as pnd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier
import seaborn as sebo
import matplotlib.pyplot as plot
import numpy as nump

# Loading the data using pandas library
data = pnd.read_csv('Indian Liver Patient Dataset (ILPD).csv')

data.info()

data.describe()

# Printing no of rows and columns in the dataset
rows = data.shape[0]
cols = data.shape[1]
print(rows)
print(cols)

data.head()

# From the header we get to know that the Gender column have 'Female' and 'Male' strings. So, instead of the string we are encoding the data with 0 or 1 using lable encoder.
# For Female = 0
# For Male = 1
encoder = LabelEncoder()
data['Gender'] = encoder.fit_transform(data['Gender'])
data.head()
print(data)

# Displaying the correlation of features
correlation = data.corr()
plot.figure(figsize=(12, 6))
sebo.heatmap(correlation, annot=True, cmap='PRGn')
plot.title('Features Correlation')
plot.show()

# Printing missing values in the dataset
print("No of missing value")
print(data.isnull().sum())
# Printing the missing value rows in the dataset
print("Missing values rows")
print(data[data.isnull().any(axis = 1)])

nonNullDataOfALBAG = data.dropna(subset=['ALB', 'A/G Ratio'])

# Calculate the correlation between 'ALB' and 'A/G Ratio'
correlationBwtALBAG = nonNullDataOfALBAG['ALB'].corr(nonNullDataOfALBAG['A/G Ratio'])

def addMissingValues(row):
    if pnd.isnull(row['A/G Ratio']):
        return row['ALB'] * correlationBwtALBAG
    return row['A/G Ratio']


data['A/G Ratio'] = data.apply(addMissingValues, axis=1)

data.info()

imputer = SimpleImputer(strategy='mean')
features = imputer.fit_transform(data.drop('Selector', axis=1))
selector = data['Selector']

# Dividing the data into train and test datasets
XtrainDS, XtestDS, YtrainDS, YtestDS = train_test_split(features, selector, test_size=0.2, random_state=42)

# Taking Random Forest Classifier model to create a model.
trainAccuracy = {}
testAccuracy = {}
model = RandomForestClassifier()
model.fit(XtrainDS, YtrainDS)
YpredTrain = model.predict(XtrainDS)
YpredTest = model.predict(XtestDS)
trainAccuracy["Random Forest"] = accuracy_score(YtrainDS, YpredTrain)
testAccuracy["Random Forest"] = accuracy_score(YtestDS, YpredTest)
accuracy = accuracy_score(YtestDS, YpredTest)
print(f"Random Forest Accuracy: {accuracy:.10f}")



import seaborn as sns
# Fetching the important features from the model.
importances = model.feature_importances_

# Transforming the features into data frame
feature_importances = pnd.DataFrame({"feature": data.columns[:-1], "importance": importances})

# Sorting the features based on importance
feature_importances = feature_importances.sort_values("importance", ascending=False)
# Define the color palette
palette = sns.color_palette("seismic", len(feature_importances))
# Demonstrate the important features using bar chart
plot.figure(figsize=(12, 6))
sebo.barplot(data=feature_importances, x="importance", y="feature", palette=palette)
plot.title('Feature Importance')
plot.show()

# Finding accuracy with different classifiers
classifiers = {
    "Decision Tree": DecisionTreeClassifier(),
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier()
}

for name, classifier in classifiers.items():
    classifier.fit(XtrainDS, YtrainDS)
    YpredTrain = classifier.predict(XtrainDS)
    YpredTest = classifier.predict(XtestDS)
    trainAccuracy[name] = accuracy_score(YtrainDS, YpredTrain)
    testAccuracy[name] = accuracy_score(YtestDS, YpredTest)
    accuracy = accuracy_score(YtestDS, YpredTest)
    print(f"{name} Accuracy: {accuracy:.10f}")

print(trainAccuracy)
print(testAccuracy)

# Converting the dictionary to dataframe and rename the column 0 to Train_Data
trainDf = pnd.DataFrame.from_dict(trainAccuracy, orient = 'index')
trainDf = trainDf.rename(columns = {0:'Train_Data'})

# Converting the dictionary to dataframe and rename the column 0 to Test_Data
testDf = pnd.DataFrame.from_dict(testAccuracy, orient = 'index')
testDf = testDf.rename(columns = {0:'Test_Data'})

# Preparing the pyplot for comparison
figure,axis = plot.subplots(figsize = (14,5))

# Converting the names in train dataframe to list
trainDataList = trainDf.index.to_list()
# Converting the first column in train dataframe to list
trainValues = trainDf.iloc[:,0].to_list()
# Converting the first column in test dataframe to list
testValues = testDf.iloc[:,0].to_list()
# Creates an array containing integers from 0 to the length of trainDataList minus 1
xarray = nump.arange(len(trainDataList))
# Specifing the width of each bar
width = 0.20

# Creating train and test bar charts
trainBars = axis.bar(x = xarray - width/2, height = trainValues, width = width, label = 'Train Accuracy')
testBars = axis.bar(x = xarray + width/2, height = testValues, width = width, label = 'Test Accuracy')

# Iterate to each bar in train collection and retrive the height and annotate it.
for trainBar in trainBars:
  trainHeight = trainBar.get_height()
  trainBar.set_color('darkcyan')
  axis.annotate(text = f'{trainHeight:.5f}', xy = (trainBar.get_x() + trainBar.get_width()/2, trainHeight), xytext = (0,3), textcoords = "offset pixels", va = "bottom", ha = "center")

# Iterate to each bar in test collection and retrive the height and annotate it.
for testBar in testBars:
  testHeight = testBar.get_height()
  testBar.set_color('coral')
  axis.annotate(text = f'{testHeight:.5f}', xy = (testBar.get_x() + testBar.get_width()/2, testHeight), xytext = (0,3), textcoords = "offset pixels", va = "bottom", ha = "center")

# Adding legend to the bar chart
axis.legend()
# Adding title to the bar chart
axis.set_title("Accuracy chart for different classifiers", fontsize = 14, fontweight = "bold", color = "black")
# Adding x axis label and data
axis.set_xlabel("Classifier Name", fontsize = 10, fontweight = "bold", color = "black")
axis.set_xticks(xarray)
axis.set_xticklabels(trainDataList)
# Adding y axis label and data
axis.set_ylabel("Accuracy", fontsize = 10, fontweight = "bold", color = "black")
# Displaying the bar chart
figure.show()

# Rebalancing the selector.
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import OneHotEncoder

features = data.drop(['Selector'], axis=1)
selector = data['Selector']

smote = SMOTE(random_state=42)

Xrebalance, Yrebalance = smote.fit_resample(features, selector)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 4))

plt.subplot(1, 2, 1)
sns.countplot(x='Selector', data=data, palette='Set1')
plt.title('Selector value before rebalance')
plt.xlabel('Selector')
plt.ylabel('No of selector')

plt.subplot(1, 2, 2)
sns.countplot(x=Yrebalance, palette='Set2')
plt.title('Selector value after rebalance')
plt.xlabel('Selector')
plt.ylabel('No of selector')

plt.tight_layout()
plt.show()

XtrainRebalanceDS, XtestRebalanceDS, YtrainRebalanceDS, YtestRebalanceDS = train_test_split(Xrebalance, Yrebalance, test_size=0.2, random_state=42)

# Taking Random Forest Classifier model to test the rebalance data.
model = RandomForestClassifier()
model.fit(XtrainRebalanceDS, YtrainRebalanceDS)
YrebalancePred = model.predict(XtestRebalanceDS)
accuracy_score(YtestRebalanceDS, YrebalancePred)
